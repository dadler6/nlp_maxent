{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Data Mexent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classifier that will utilize the Stanford data found online and my created <code>maxent_sentiment_analysis.py</code> module to build a maximum entropy classifier.\n",
    "\n",
    "The data used from this code can be found <a href=\"http://help.sentiment140.com/for-students/\" target = \"_blank\">here</a>, and the original paper is <a href=\"http://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\" target = \"_blank\">here</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First upload data\n",
    "\n",
    "First I will import the object from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the file\n",
    "from maxent_sentiment_analysis import MaxEntSentimentClassifier as maxent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now add data\n",
    "\n",
    "I stored the data in three files within the \"Stanford_data folder\":\n",
    "\n",
    "1. <em>Stanford_data\\\\training_data.txt</em>\n",
    "2. <em>Stanford_data\\\\training_classifier.txt</em>\n",
    "3. <em>Stanford_data\\\\test_tweets.txt</em>\n",
    "\n",
    "We can add these files in three modules below and upload the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_tweets = \"Stanford_data/training_data.txt\"\n",
    "training_classes = \"Stanford_data/training_classifier.txt\"\n",
    "test_tweets = \"Stanford_data/test_tweets.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the object and upload data\n",
    "\n",
    "\n",
    "### Create the object\n",
    "\n",
    "We will now create a <code>MaxEntSentimentClassifier</code> by utilzing the <code>maxent</code> module imported above. This can be done simply by saying:\n",
    "\n",
    "classifier = maxent()\n",
    "\n",
    "### Upload data\n",
    "\n",
    "Next we can add the training data, which composes of the tweets themselves and then the classification for each tweet.  There are two functions that are created to handle importing the data in the two text files:\n",
    "\n",
    "1. <code>MaxEntSentimentClassifier.add_tweets(filename)</code>\n",
    "2. <code>MaxEntSentimentClassifier.add_classification_data(filename)</code>\n",
    "\n",
    "The data MUST be in a <em>.txt</em> format and MUST be listed such that each tweet is a row, and the classificaiton for that tweet is on the corresponding row in the second text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Create object\n",
    "classifier = maxent()\n",
    "\n",
    "# Upload files\n",
    "classifier.add_tweets(training_tweets)\n",
    "classifier.add_classification_data(training_classes)\n",
    "\n",
    "# Check to see that everything uploaded currently\n",
    "print(len(classifier.tweets))\n",
    "print(len(classifier.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data\n",
    "\n",
    "Now one can train the classifier.  This is handled by the function\n",
    "\n",
    "<code>MaxEntSentimentClassifier.train_data()</code>\n",
    "\n",
    "This function will first organized the data in a format handled by NLTK.  This format introduced a list where each entry is a pair.  The first entry in the pair is a dictionary that shows the frequency of words per tweet. My object utilizes a python <code>Counter()</code> object to create this.  A <code>Counter()</code> is simply a child of a dictionary, so NLTK can handle this.  The second is simply the classification for that tweet.\n",
    "\n",
    "As of 9/9/2016 I also added cleaning steps.  The cleaning steps are in the <code>__clean_data(self, s)</code> function.  Right now I clean by:\n",
    "\n",
    "1. Lower casing the tweet\n",
    "2. Removing all URLs\n",
    "3. Turning all characters occurring more than twice into a two time occurance\n",
    "   - Ex: \"haaaaapppy\" => \"haappy\"\n",
    "4. Removing usernames from the data (i.e. anything beginning with an @ symbol)\n",
    "5. Removing all remaining punctuation\n",
    "\n",
    "This cleaning was supposed to mimic the cleaning in the original Stanford paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.55704        0.920\n",
      "             3          -0.47102        0.943\n",
      "             4          -0.41173        0.951\n",
      "             5          -0.36808        0.957\n",
      "             6          -0.33434        0.960\n",
      "             7          -0.30731        0.965\n",
      "             8          -0.28504        0.968\n",
      "             9          -0.26630        0.970\n",
      "            10          -0.25027        0.972\n",
      "            11          -0.23637        0.974\n",
      "            12          -0.22416        0.976\n",
      "            13          -0.21335        0.978\n",
      "            14          -0.20369        0.979\n",
      "            15          -0.19500        0.980\n",
      "            16          -0.18712        0.980\n",
      "            17          -0.17996        0.981\n",
      "            18          -0.17340        0.981\n",
      "            19          -0.16737        0.982\n",
      "            20          -0.16181        0.983\n",
      "            21          -0.15666        0.984\n",
      "            22          -0.15188        0.985\n",
      "            23          -0.14742        0.985\n",
      "            24          -0.14325        0.987\n",
      "            25          -0.13935        0.987\n",
      "            26          -0.13568        0.987\n",
      "            27          -0.13223        0.988\n",
      "            28          -0.12898        0.988\n",
      "            29          -0.12591        0.989\n",
      "            30          -0.12300        0.989\n",
      "            31          -0.12024        0.989\n",
      "            32          -0.11762        0.989\n",
      "            33          -0.11513        0.990\n",
      "            34          -0.11275        0.990\n",
      "            35          -0.11049        0.990\n",
      "            36          -0.10833        0.990\n",
      "            37          -0.10626        0.990\n",
      "            38          -0.10429        0.990\n",
      "            39          -0.10239        0.991\n",
      "            40          -0.10058        0.991\n",
      "            41          -0.09883        0.991\n",
      "            42          -0.09716        0.991\n",
      "            43          -0.09554        0.991\n",
      "            44          -0.09399        0.992\n",
      "            45          -0.09250        0.992\n",
      "            46          -0.09106        0.992\n",
      "            47          -0.08967        0.992\n",
      "            48          -0.08832        0.992\n",
      "            49          -0.08703        0.992\n",
      "            50          -0.08577        0.992\n",
      "            51          -0.08456        0.992\n",
      "            52          -0.08339        0.992\n",
      "            53          -0.08225        0.992\n",
      "            54          -0.08115        0.992\n",
      "            55          -0.08008        0.992\n",
      "            56          -0.07904        0.992\n",
      "            57          -0.07804        0.993\n",
      "            58          -0.07706        0.993\n",
      "            59          -0.07611        0.993\n",
      "            60          -0.07519        0.993\n",
      "            61          -0.07429        0.993\n",
      "            62          -0.07342        0.993\n",
      "            63          -0.07257        0.993\n",
      "            64          -0.07174        0.993\n",
      "            65          -0.07093        0.994\n",
      "            66          -0.07015        0.994\n",
      "            67          -0.06938        0.994\n",
      "            68          -0.06863        0.994\n",
      "            69          -0.06790        0.994\n",
      "            70          -0.06719        0.994\n",
      "            71          -0.06650        0.994\n",
      "            72          -0.06582        0.994\n",
      "            73          -0.06516        0.994\n",
      "            74          -0.06451        0.994\n",
      "            75          -0.06388        0.994\n",
      "            76          -0.06326        0.994\n",
      "            77          -0.06266        0.994\n",
      "            78          -0.06207        0.994\n",
      "            79          -0.06149        0.994\n",
      "            80          -0.06093        0.994\n",
      "            81          -0.06037        0.994\n",
      "            82          -0.05983        0.994\n",
      "            83          -0.05930        0.994\n",
      "            84          -0.05878        0.994\n",
      "            85          -0.05827        0.994\n",
      "            86          -0.05777        0.994\n",
      "            87          -0.05728        0.994\n",
      "            88          -0.05680        0.994\n",
      "            89          -0.05633        0.994\n",
      "            90          -0.05586        0.994\n",
      "            91          -0.05541        0.994\n",
      "            92          -0.05496        0.994\n",
      "            93          -0.05453        0.994\n",
      "            94          -0.05410        0.994\n",
      "            95          -0.05368        0.994\n",
      "            96          -0.05326        0.994\n",
      "            97          -0.05286        0.994\n",
      "            98          -0.05246        0.994\n",
      "            99          -0.05207        0.994\n",
      "         Final          -0.05168        0.994\n"
     ]
    }
   ],
   "source": [
    "# Classify the data based upon loaded train data\n",
    "classifier.train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see how well we did\n",
    "\n",
    "### Upload the test data\n",
    "\n",
    "This can be done with the function:\n",
    "\n",
    "<code>MaxEntSentimentClassifier.add_test_data(filename)</code>\n",
    "\n",
    "We already saved the file path in a test_tweets variable.\n",
    "\n",
    "### Classify the test data\n",
    "\n",
    "We can now classify the test data.  The classification of each tweet will be saved to a list and return from a function.  THe following function:\n",
    "\n",
    "<code>MaxEntSentimentClassifer.classify_test_data()</code>\n",
    "\n",
    "will classify the data and print to standard out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '4', '4', '4', '0', '4', '0', '4', '4', '4', '0', '4', '0', '4', '4', '0', '0', '4', '0', '0', '4', '4', '0', '4', '0', '4', '0', '4', '0', '4', '4', '4', '4', '0', '0', '0', '0', '0', '0', '4', '4', '0', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '0', '0', '4', '0', '0', '4', '4', '0', '0', '4', '4', '4', '4', '4', '4', '0', '0', '0', '0', '4', '0', '4', '0', '4', '4', '0', '0', '0', '4', '4', '4', '4', '4', '4', '0', '4', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '4', '0', '4', '4', '4', '4', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '0', '4', '0', '0', '4', '0', '4', '0', '0', '4', '4', '0', '4', '4', '0', '0', '4', '0', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0', '0', '0', '4', '0', '0', '0', '0', '0', '0', '4', '4', '0', '4', '0', '4', '0', '4', '4', '4', '4', '4', '0', '0', '4', '4', '0', '4', '0', '4', '4', '0', '0', '0', '4', '4', '4', '4', '4', '0', '4', '4', '0', '4', '4', '0', '4', '0', '4', '4', '4', '0', '4', '4', '4', '0', '0', '0', '0', '4', '4', '0', '0', '4', '4', '4', '4', '0', '4', '4', '4', '0', '0', '0', '4', '0', '4', '4', '0', '0', '0', '4', '4', '4', '4', '4', '0', '4', '4', '4', '4', '4', '4', '0', '4', '0', '0', '4', '4', '0', '0', '4', '0', '0', '0', '0', '4', '4', '4', '4', '0', '0', '4', '4', '4', '4', '4', '0', '4', '0', '0', '0', '0', '4', '0', '4', '4', '0', '0', '4', '0', '0', '4', '0', '4', '0', '0', '0', '4', '4', '4', '0', '4', '4', '0', '0', '4', '0', '4', '4', '0', '4', '4', '4', '4', '0', '4', '4', '0', '4', '0', '4', '0', '4', '0', '4', '4', '0', '0', '4', '4', '4', '0', '0', '4', '4', '4', '0', '4', '0', '4', '0', '0', '0', '0', '4', '4', '4', '4', '4', '0', '0', '0', '4', '4', '4', '0', '4', '4', '4', '4', '4', '0', '4', '4', '4', '4', '4', '0', '4', '4', '0', '0', '4', '4', '4', '4', '4', '0', '0', '4', '4', '4', '4', '0', '4', '0', '0', '4', '0', '0', '0', '0', '0', '4', '0', '4', '4', '0', '0', '0', '4', '0', '0', '0', '0', '4', '0', '0', '0', '0', '4', '0', '4', '0', '0', '0', '0', '0', '0', '0', '4', '4', '4', '0', '0', '0', '4', '0', '4', '0', '4', '0', '0', '4', '4', '0', '0', '4', '4', '4', '4', '4', '4', '4', '0', '0', '4', '0', '4', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '4', '0', '0', '4', '0', '0', '0', '4', '4', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '0', '4', '4', '4', '0', '0', '0', '0', '4', '4', '4', '0', '4', '0', '0', '0', '4', '4', '4', '4', '0', '0', '0']\n",
      "['0', '4', '4', '4', '0', '4', '0', '4', '4', '4', '0', '4', '0', '4', '4', '0', '0', '4', '0', '0', '4', '4', '0', '4', '0', '4', '0', '4', '0', '4', '4', '4', '4', '0', '0', '0', '0', '0', '0', '4', '4', '0', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '0', '0', '4', '0', '0', '4', '4', '0', '0', '4', '4', '4', '4', '4', '4', '0', '0', '0', '0', '4', '0', '4', '0', '4', '4', '0', '0', '0', '4', '4', '4', '4', '4', '4', '0', '4', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '4', '0', '4', '4', '4', '4', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '0', '4', '0', '0', '4', '0', '4', '0', '0', '4', '4', '0', '4', '4', '0', '0', '4', '0', '4', '0', '0', '0', '0', '0', '0', '0', '0', '4', '0', '0', '0', '4', '0', '0', '0', '0', '0', '0', '4', '4', '0', '4', '0', '4', '0', '4', '4', '4', '4', '4', '0', '0', '4', '4', '0', '4', '0', '4', '4', '0', '0', '0', '4', '4', '4', '4', '4', '0', '4', '4', '0', '4', '4', '0', '4', '0', '4', '4', '4', '0', '4', '4', '4', '0', '0', '0', '0', '4', '4', '0', '0', '4', '4', '4', '4', '0', '4', '4', '4', '0', '0', '0', '4', '0', '4', '4', '0', '0', '0', '4', '4', '4', '4', '4', '0', '4', '4', '4', '4', '4', '4', '0', '4', '0', '0', '4', '4', '0', '0', '4', '0', '0', '0', '0', '4', '4', '4', '4', '0', '0', '4', '4', '4', '4', '4', '0', '4', '0', '0', '0', '0', '4', '0', '4', '4', '0', '0', '4', '0', '0', '4', '0', '4', '0', '0', '0', '4', '4', '4', '0', '4', '4', '0', '0', '4', '0', '4', '4', '0', '4', '4', '4', '4', '0', '4', '4', '0', '4', '0', '4', '0', '4', '0', '4', '4', '0', '0', '4', '4', '4', '0', '0', '4', '4', '4', '0', '4', '0', '4', '0', '0', '0', '0', '4', '4', '4', '4', '4', '0', '0', '0', '4', '4', '4', '0', '4', '4', '4', '4', '4', '0', '4', '4', '4', '4', '4', '0', '4', '4', '0', '0', '4', '4', '4', '4', '4', '0', '0', '4', '4', '4', '4', '0', '4', '0', '0', '4', '0', '0', '0', '0', '0', '4', '0', '4', '4', '0', '0', '0', '4', '0', '0', '0', '0', '4', '0', '0', '0', '0', '4', '0', '4', '0', '0', '0', '0', '0', '0', '0', '4', '4', '4', '0', '0', '0', '4', '0', '4', '0', '4', '0', '0', '4', '4', '0', '0', '4', '4', '4', '4', '4', '4', '4', '0', '0', '4', '0', '4', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '4', '0', '0', '4', '0', '0', '0', '4', '4', '0', '0', '0', '0', '4', '0', '4', '4', '4', '4', '4', '0', '0', '4', '4', '4', '0', '0', '0', '0', '4', '4', '4', '0', '4', '0', '0', '0', '4', '4', '4', '4', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "# Add data\n",
    "classifier.add_test_data(test_tweets)\n",
    "# Classify data\n",
    "test_classes = classifier.classify_test_data()\n",
    "print(test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy\n",
    "\n",
    "Let's see how well we did.  The true classification results are in\n",
    "\n",
    "<em>Stanford_data\\\\test_true_classification.txt</em>\n",
    "\n",
    "Again, each row is listed with whether it is a positive (0) or negative (4) tweet.\n",
    "\n",
    "Some tweets are marked as neutral (2) so we will have to filter out those.  We'll make the following variables:\n",
    "\n",
    "1. TP = True Positive. Positive = \"0\"\n",
    "2. TN = True Negative. Negative = \"4\"\n",
    "3. FP = False Positive\n",
    "4. FN = False Negative.\n",
    "5. Total = total results\n",
    "\n",
    "to get a confusion matrix of the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 0.30919220055710306\n",
      "True Negative: 0.3342618384401114\n",
      "False Positive: 0.17270194986072424\n",
      "False Negative: 0.18384401114206128\n",
      "Accuracy: 0.6434540389972145\n",
      "Precision: 0.6416184971098265\n",
      "Recall: 0.6271186440677966\n"
     ]
    }
   ],
   "source": [
    "# Open file\n",
    "true_classes = open(\"Stanford_data/test_true_classification.txt\", \"r\").read().split('\\n')\n",
    "\n",
    "# Make vars\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(test_classes)):\n",
    "    if test_classes[i] == '0' and true_classes[i] == '0':\n",
    "        TP += 1\n",
    "    elif test_classes[i] == '0' and true_classes[i] == '4':\n",
    "        FP += 1\n",
    "    elif test_classes[i] == '4' and true_classes[i] == '4':\n",
    "        TN += 1\n",
    "    elif test_classes[i] == '4' and true_classes[i] == '0':\n",
    "        FN += 1\n",
    "\n",
    "Total = (TP + TN + FP + FN)\n",
    "Accuracy = (TP + TN)/Total\n",
    "Recall = TP/(TP + FN)\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "print(\"True Positive: \" + str(TP/Total))\n",
    "print(\"True Negative: \" + str(TN/Total))\n",
    "print(\"False Positive: \" + str(FP/Total))\n",
    "print(\"False Negative: \" + str(FN/Total))\n",
    "print(\"Accuracy: \" + str(Accuracy))\n",
    "print(\"Precision: \" + str(Precision))\n",
    "print(\"Recall: \" + str(Recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "Other than taking out punctuation, there was no filtering or these tweets.  The Stanford paper took out all emojis, RT's and usernames (defind by \"@\") symbol.  More filtering wil be necessary in the future.\n",
    "\n",
    "As of 09/09/2016 the filtering has been completed.  Accuracy did not improve that much...\n",
    "Thinking I will now try to use more data, but need to think of the best way to store that data.  5000 tweets is not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
